{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on Activity 8.1: Aggregating Data with Pandas\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lpIqC0MvOYSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# Im following the Modules and yet I receive future warnings so I added this..."
      ],
      "metadata": {
        "id": "3e-rrFVNAuMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.1 Intended Learning Outcomes"
      ],
      "metadata": {
        "id": "kw9-gN3tOygP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this activity, the student should be able to:\n",
        "- Demonstrate querying and merging of dataframes\n",
        "- Perform advanced calculations on dataframes\n",
        "- Aggregate dataframes with pandas and numpy\n",
        "- Work with time series data"
      ],
      "metadata": {
        "id": "WQtNGmYcO2zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.2 Resources"
      ],
      "metadata": {
        "id": "ZZ424zlVO-Lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Computing Environment using Python 3.x\n",
        "- Attached Datasets (under Instructional Materials)"
      ],
      "metadata": {
        "id": "SWwcwzcnPBmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.3 Procedures"
      ],
      "metadata": {
        "id": "6gPSKTEDPF6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedures can be found in the canvas module. Check the following under topics:\n",
        "- 8.1 Weather Data Collection\n",
        "- 8.2 Querying and Merging\n",
        "- 8.3 Dataframe Operations\n",
        "- 8.4 Aggregations\n",
        "- 8.5 Time Series"
      ],
      "metadata": {
        "id": "VuNHGq1RPImy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.4 Data Analysis\n"
      ],
      "metadata": {
        "id": "xD2FeFsPPPsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSIGHTS AND COMMENTS IN MODULE 1:\n",
        "\n",
        "This Module is the same as the first activity of the last moddule, when we are collecting data form an API (Appicatin Programming Interface).\n",
        "This API or the 'National Oceanic and Atmospheric Administration' NCDC API is the one that being used to this modue.\n",
        "\n",
        "**First I've learned to understand and read Documentation of a certain API or the items provided to give you knowledge about the software you want to use**\n",
        "- if i don't know or understand what to do, I refer first to the documentation before going to Internet or an AI.\n",
        "- think of it as a manual even though some documentation has some overwhelming infoormation to throw at you.\n",
        "\n",
        "## INSIGHTS AND COMMENTS IN MODULE 2\n",
        "\n",
        "Querying in Pandas/Dataframes are easy and easy to remember because of it's similarity in SQL.\n",
        "\n",
        "**I've Learned that there are two ways to et the query you like/desire**:\n",
        "1. The Easy DataFrame.query():\n",
        "   - The query accepts a SQL script as a Parameter.\n",
        "   - ![image](https://github.com/KurtyMittens/CPE311_CTwithPy/assets/134671520/efa2b090-3238-42fe-9b61-3775699274c8)\n",
        "\n",
        "2. the bitwise Opt Cracker, The Booolean Mask :\n",
        "   - uses bitwise to get what he/she wanted/desired in the query\n",
        "   -  ![image](https://github.com/KurtyMittens/CPE311_CTwithPy/assets/134671520/3759258b-bd2a-48d9-a69a-a482493b2943)\n",
        "\n",
        "**Merging folllows the JOIN in SQL**\n",
        "- using DataFrame.merge() we can perform Joins such as left, right, inner joins and many more. (Inner Join as the default)\n",
        "- ![image](https://github.com/KurtyMittens/CPE311_CTwithPy/assets/134671520/46cbd53a-6cb3-4908-943f-5a1fac9cf026)\n",
        "- ![image](https://github.com/KurtyMittens/CPE311_CTwithPy/assets/134671520/b6342607-77e7-452a-9590-2fb752da8c3d)\n",
        "\n",
        "## INSIGHTS AND COMMENTS FOR MODULE 3\n",
        "\n",
        "This module consists of the basic Operations you can do to your dataframes. The most memorable is these:\n",
        "\n",
        "## pct_change()\n",
        "- lets you change the current number of a column to its percentage equivalent.\n",
        "\n",
        "## pd.cut(), pd.qcut()\n",
        "- let youu create seperate bins in order for you to identify the rages/roupings of your data\n",
        "\n",
        "## .apply()\n",
        "- lets you apply Functions in certain dataframe or specific coluumn for an easy configurations\n",
        "\n",
        "## Window calculations\n",
        "- rolinng calculations can be done using this <-\n",
        "\n",
        "## INSIGHTS AND COMMENTS FOR MODULE 4\n",
        "\n",
        "this Aggeration of dataframe is simply formating the data for future use.\n",
        "can be done uing the agg() function.\n",
        "\n",
        "but this module alo includes:\n",
        "## group_by()\n",
        "- aggreating the groups of a dataframe not the entire dataframe\n",
        "\n",
        "## pivot_tables\n",
        "- Provides the simmplest form of aggregation\n",
        "\n",
        "## crosstabs\n",
        "- in which you get the frequency of a specific column\n",
        "\n",
        "## INSIGHTS AND COMMENTS IN MODDULE 5\n",
        "\n",
        "- basically this module talks about time. Yo can calulate time wiht TimeDelta() andd you can select data from simply inputting the dates/range of dates of the desired data.\n",
        "\n",
        "- You can also resample your data by the DateTime Index, Period Index, and timedeltaIndex\n"
      ],
      "metadata": {
        "id": "xIe8bYeiPS5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.5 Supplementary Activity"
      ],
      "metadata": {
        "id": "ExrmO3pFPUxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the CSV files provided and what we have learned so far in this module complete the following exercises"
      ],
      "metadata": {
        "id": "EOOWWesZPYFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "earthquake_df = pd.DataFrame(pd.read_csv(\"/content/earthquakes.csv\")) # reading and making a dtaframe from earthquake.csv\n",
        "faang_df = pd.DataFrame(pd.read_csv(\"/content/faang.csv\", parse_dates=['date'], index_col = ['date'])) # same to this!"
      ],
      "metadata": {
        "id": "hb9o0Y18PPcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_df.head(10) # earthquake dataframe"
      ],
      "metadata": {
        "id": "HI4vEriGZUzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faang_df.head(10) # faang dataframe"
      ],
      "metadata": {
        "id": "86ogaR4YZYh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. With the earthquakes.csv file, select all the earthquakes in Japan with a magType of mb and a magnitude of 4.9 or greater."
      ],
      "metadata": {
        "id": "NlmGMUQKRKQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_df.query('magType == \"mb\" and mag > 4.9 and parsed_place == \"Japan\"') # Simple method"
      ],
      "metadata": {
        "id": "EBv8wsxISiyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_df[\n",
        "    (earthquake_df['magType'] == 'mb') & (earthquake_df['mag'] > 4.9) & (earthquake_df['parsed_place'] == 'Japan')\n",
        "    ] # Boolean Mask method"
      ],
      "metadata": {
        "id": "_y1y08w5RJaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create bins for each full number of magnitude (for example, the first bin is 0-1, the second is 1-2, and so on) with a magType of ml and count how many are in each bin."
      ],
      "metadata": {
        "id": "eoJF8e2ASVsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_magBin = pd.cut(earthquake_df.query('magType == \"ml\"').mag,\n",
        "                           bins=6,\n",
        "                           labels=['0-1', '1-2', '2-3', '3-4', '4-5', '5-6'])\n",
        "earthquake_magBin.value_counts() # Creating bins"
      ],
      "metadata": {
        "id": "YguT72-DWJ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using the faang.csv file, group by the ticker and resample to monthly frequency. Make the following aggregations"
      ],
      "metadata": {
        "id": "4W5e6JOZY13c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mean of the opening price\n",
        "- Maximum of the high price\n",
        "- Minimum of the low price\n",
        "- Mean of the closing price\n",
        "- Sum of the volume traded"
      ],
      "metadata": {
        "id": "LbefBcBZZKEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aggFang = faang_df.groupby(\"ticker\").agg( # agg() does the job for aggregation\n",
        "    {\n",
        "        'open': np.mean,  # Self explainatory things is descriptive statictics\n",
        "        'high': np.max,\n",
        "        'low': np.min,\n",
        "        'close': np.mean,\n",
        "        'volume': np.sum\n",
        "    }\n",
        ")\n",
        "aggFang"
      ],
      "metadata": {
        "id": "usEjC3jNZJlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Build a crosstab with the earthquake data between the tsunami column and the magType column. Rather than showing the frequency count, show the maximum\n",
        "magnitude that was observed for each combination. Put the magType along the columns."
      ],
      "metadata": {
        "id": "9f_yUctIbzFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(\n",
        "    index=earthquake_df.tsunami,\n",
        "    columns=earthquake_df.magType,\n",
        "    values=earthquake_df.mag, # As said, instead of the frequency, we get the Magnitude of the combination\n",
        "    aggfunc=lambda x:np.max(x)\n",
        ").fillna(0) # since there are Empty shells, it is better to fill it up"
      ],
      "metadata": {
        "id": "KsNo8e1nbyUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Calculate the rolling 60-day aggregations of OHLC data by ticker for the FAANG data. Use the same aggregations as exercise no. 3."
      ],
      "metadata": {
        "id": "7771NVDIYJZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faang_df.groupby(['ticker']).rolling('60D').agg( # Grouuping them by the Ticker and setting Rolling for 60 Days\n",
        "    {\n",
        "        'open': np.mean,\n",
        "        'high': np.max,\n",
        "        'low': np.min,\n",
        "        'close': np.mean,\n",
        "        'volume': np.sum\n",
        "    }\n",
        "\n",
        ").join(aggFang[['open', # Join the past aggregation for a comparison of the data\n",
        "        'high',\n",
        "        'low',\n",
        "        'close',\n",
        "        'volume']], lsuffix=\"_rolling\").sort_index(axis=1) # adding suffixes to prevent errors"
      ],
      "metadata": {
        "id": "q_oe-E1eoEYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a pivot table of the FAANG data that compares the stocks. Put the ticker in the rows and show the averages of the OHLC and volume traded data."
      ],
      "metadata": {
        "id": "sjEJA8ED12Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faang_df.pivot_table(\n",
        "  index = 'ticker',\n",
        "  values = ['open', 'high', 'low','close', 'volume'],\n",
        "  aggfunc = 'mean' # getting the mean == average\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "55Uv4NOu16o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Calculate the Z-scores for each numeric column of Netflix's data (ticker is NFLX) using apply()."
      ],
      "metadata": {
        "id": "oLubRzDe345F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = faang_df.columns[1:]\n",
        "\n",
        "for i in columns:\n",
        "  faang_df[f'{i}_zscore'] = faang_df[f'{i}'].apply(\n",
        "      lambda x: (x - (faang_df.mean()[f'{i}'])) / (faang_df.std()[f'{i}'])\n",
        "      ).abs()\n",
        "\n",
        "faang_df.query('ticker == \"NFLX\"')"
      ],
      "metadata": {
        "id": "DeGmo3ZK7u5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Add event descriptions:\n",
        "- Create a dataframe with the following three columns: ticker, date, and event. The columns should have the following values:\n",
        "  - ticker: 'FB'\n",
        "  - date: ['2018-07-25', '2018-03-19', '2018-03-20']\n",
        "  - event: ['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']\n",
        "- Set the index to ['date', 'ticker']\n",
        "- Merge this data with the FAANG data using an outer join"
      ],
      "metadata": {
        "id": "OqU7PVlbLOAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_df = pd.DataFrame({\n",
        "    'ticker':'FB',\n",
        "    'date':[\"2018-07-25\", \"2018-03-19\", \"2018-03-20\"],\n",
        "    'event':['Disappointing user growth announced after close.',\n",
        "             'Cambridge Analytica story', 'FTC investigation']\n",
        "})"
      ],
      "metadata": {
        "id": "0QOSjjb337na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faang_df = pd.DataFrame(pd.read_csv(\"/content/faang.csv\"))\n",
        "faang_df.merge(event_df, how = 'outer')"
      ],
      "metadata": {
        "id": "SvYdH62imXIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Use the transform() method on the FAANG data to represent all the values in terms of the first date in the data. To do so, divide all the values for each ticker by the values\n",
        "for the first date in the data for that ticker. This is referred to as an index, and the data for the first date is the base (https://ec.europa.eu/eurostat/statistics-explained/\n",
        "index.php/ Beginners:Statisticalconcept-Indexandbaseyear). When data is in this format, we can easily see growth over time. Hint: transform() can take a function name."
      ],
      "metadata": {
        "id": "MLQA3lCKOH3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faang_df.groupby('ticker').head()"
      ],
      "metadata": {
        "id": "e4j0VHZ0OPKu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}