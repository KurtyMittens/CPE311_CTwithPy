{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregations with pandas and numpy"
      ],
      "metadata": {
        "id": "lrcaZQJR0cwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the Data"
      ],
      "metadata": {
        "id": "pU7fqG8d0fgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will be working with 2 data sets:\n",
        "- Facebook's stock price throughout 2018 (obtained using the stock_analysis package).\n",
        "- daily weather data for NYC from the National Centers for Environmental Information (NCEI) API.\n",
        "\n",
        "Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the\n",
        "NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for the NCEI weather API to find the updated one."
      ],
      "metadata": {
        "id": "m-IlSvCg0h-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background on the weather data"
      ],
      "metadata": {
        "id": "AudQigRP0nHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data meanings:\n",
        "- AWND : average wind speed\n",
        "- PRCP : precipitation in millimeters\n",
        "- SNOW : snowfall in millimeters\n",
        "- SNWD : snow depth in millimeters\n",
        "- TMAX : maximum daily temperature in Celsius\n",
        "- TMIN : minimum daily temperature in Celsius"
      ],
      "metadata": {
        "id": "dOUQU9tS0pfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "YnjvGvcY0usl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-ZCCRPGxEZz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "weather = pd.read_csv('/content/weather_by_station.csv', index_col='date', parse_dates=True)\n",
        "weather.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fb = pd.read_csv('/content/fb_2018.csv', index_col='date', parse_dates=True).assign(\n",
        "trading_volume=lambda x: pd.cut(x.volume, bins=3, labels=['low', 'med', 'high'])\n",
        ")\n",
        "fb.head()"
      ],
      "metadata": {
        "id": "tjuUs9iJ3IXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into any calculations, let's make sure pandas won't put things in scientific notation. We will modify how floats are formatted for displaying. The format we will\n",
        "apply is .2f , which will provide the float with 2 digits after the decimal point:"
      ],
      "metadata": {
        "id": "K_avs-Zq3NmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
      ],
      "metadata": {
        "id": "6-TEH6Ox3KR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarizing DataFrames"
      ],
      "metadata": {
        "id": "2d2iw2r14lFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learned about agg() in the dataframe operations notebook when we learned about window calculations; however, we can call this on the dataframe directly to\n",
        "aggregate its contents into a single series:"
      ],
      "metadata": {
        "id": "vTQ8eyXw4mlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.agg({\n",
        "  'open': np.mean,\n",
        "  'high': np.max,\n",
        "  'low': np.min,\n",
        "  'close': np.mean,\n",
        "  'volume': np.sum\n",
        "})"
      ],
      "metadata": {
        "id": "bpkrjHO64n8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this to find the total snowfall and precipitation recorded in Central Park in 2018:"
      ],
      "metadata": {
        "id": "DK4-Gvyl4r3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather.query(\n",
        "  'station == \"GHCND:USW00094728\"'\n",
        ").pivot(columns='datatype', values='value')[['SNOW', 'PRCP']].sum()"
      ],
      "metadata": {
        "id": "47SpxXeL4sI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is equivalent to passing 'sum' to agg()"
      ],
      "metadata": {
        "id": "cG4KxTDe4xA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather.query(\n",
        "  'station == \"GHCND:USW00094728\"'\n",
        ").pivot(columns='datatype', values='value')[['SNOW', 'PRCP']].agg('sum')"
      ],
      "metadata": {
        "id": "EBNj1gnd4xYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we aren't limited to providing a single aggregation per column. We can pass a list, and we will get a dataframe back instead of a series. nan values are placed\n",
        "where we don't have a calculation result to display:"
      ],
      "metadata": {
        "id": "QwastrfY44C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.agg({\n",
        "  'open': 'mean',\n",
        "  'high': ['min', 'max'],\n",
        "  'low': ['min', 'max'],\n",
        "  'close': 'mean'\n",
        "})"
      ],
      "metadata": {
        "id": "0BBugjjP46as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using groupby()"
      ],
      "metadata": {
        "id": "ZYZkH_GT5F8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often we won't want to aggregate on the entire dataframe, but on groups within it. For this purpose, we can run groupby() before the aggregation. If we group by the\n",
        "trading_volume column, we will get a row for each of the values it takes on:"
      ],
      "metadata": {
        "id": "1t5TWP3-5HGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.groupby('trading_volume').mean()"
      ],
      "metadata": {
        "id": "1XH4KpaJ5Gpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we run the groupby() , we can still select columns for aggregation:"
      ],
      "metadata": {
        "id": "L3JOuaiN5X7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.groupby('trading_volume')['close'].agg(['min', 'max', 'mean'])"
      ],
      "metadata": {
        "id": "2TZcfjG05YMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can still provide a dictionary specifying the aggregations to perform, but passing a list for a column will result in a hierarchical index for the columns:"
      ],
      "metadata": {
        "id": "LkitUmiS5b4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb_agg = fb.groupby('trading_volume').agg({\n",
        "  'open': 'mean',\n",
        "  'high': ['min', 'max'],\n",
        "  'low': ['min', 'max'],\n",
        "  'close': 'mean'\n",
        "})\n",
        "fb_agg"
      ],
      "metadata": {
        "id": "LgbeS6g_5ecW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hierarchical index in the columns looks like this:"
      ],
      "metadata": {
        "id": "DYhy8Tue5mJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb_agg.columns"
      ],
      "metadata": {
        "id": "WLpbOvKP5mmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a list comprehension, we can join the levels (in a tuple) with an _ at each iteration:"
      ],
      "metadata": {
        "id": "FnB4ydn15oEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb_agg.columns = ['_'.join(col_agg) for col_agg in fb_agg.columns]\n",
        "fb_agg.head()"
      ],
      "metadata": {
        "id": "auEs-ASc5qTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can group on datetimes despite them being in the index if we use a Grouper :"
      ],
      "metadata": {
        "id": "v8gU9Hjf5vBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather['2018-10'].query('datatype == \"PRCP\"').groupby(\n",
        "  pd.Grouper(freq='D')\n",
        ").mean().head()"
      ],
      "metadata": {
        "id": "BSdbw21i5wjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Grouper can be one of many group by values. Here, we find the quarterly total precipitation per station:"
      ],
      "metadata": {
        "id": "7UBZe2dc5z9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather.query('datatype == \"PRCP\"').groupby(\n",
        "  ['station_name', pd.Grouper(freq='Q')]\n",
        ").sum().unstack().sample(5, random_state=1)"
      ],
      "metadata": {
        "id": "wW-4zkCM5yDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we can use filter() to exclude some groups from aggregation. Here, we only keep groups with 'NY' in the group's name attribute, which is the station ID in\n",
        "this case"
      ],
      "metadata": {
        "id": "5lat43HR53uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather.groupby('station').filter( # station IDs with NY in them\n",
        "  lambda x: 'NY' in x.name\n",
        ").query('datatype == \"SNOW\"').groupby('station_name').sum().squeeze() # aggregate and make a series (squeeze)"
      ],
      "metadata": {
        "id": "-v6MqiUv55qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see which months have the most precipitation. First, we need to group by day and average the precipitation across the stations. Then we can group by month and sum\n",
        "the resulting precipitation. We use nlargest() to give the 5 months with the most precipitation:"
      ],
      "metadata": {
        "id": "lo8FJcer5_Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather.query('datatype == \"PRCP\"').groupby(\n",
        "  pd.Grouper(freq='D')\n",
        ").mean().groupby(pd.Grouper(freq='M')).sum().value.nlargest()"
      ],
      "metadata": {
        "id": "sSj56nzV6BBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps the previous result was surprising. The saying goes \"April showers bring May flowers\"; yet April wasn't in the top 5 (neither was May for that matter). Snow will count\n",
        "towards precipitation, but that doesn't explain why summer months are higher than April. Let's look for days that accounted for a large percentage of the precipitation in a\n",
        "given month.\n",
        "\n",
        "In order to do so, we need to calculate the average daily precipitation across stations and then find the total per month. This will be the denominator. However, in order to\n",
        "divide the daily values by the total for their month, we will need a Series of equal dimensions. This means we will need to use transform() :"
      ],
      "metadata": {
        "id": "AgxJxzz-6DmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather.query('datatype == \"PRCP\"').rename(\n",
        "  dict(value='prcp'), axis=1\n",
        ").groupby(pd.Grouper(freq='D')).mean().groupby(\n",
        "  pd.Grouper(freq='M')\n",
        ").transform(np.sum)['2018-01-28':'2018-02-03']"
      ],
      "metadata": {
        "id": "4QZvYWmY6FP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how we have the same value repeated for each day in the month it belongs to. This will allow us to calculate the percentage of the monthly precipitation that occurred\n",
        "each day and then pull out the largest values:"
      ],
      "metadata": {
        "id": "0h_sgdEJ6J6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather\\\n",
        "  .query('datatype == \"PRCP\"')\\\n",
        "  .rename(dict(value='prcp'), axis=1)\\\n",
        "  .groupby(pd.Grouper(freq='D')).mean()\\\n",
        "  .assign(\n",
        "  total_prcp_in_month=lambda x: x.groupby(\n",
        "  pd.Grouper(freq='M')\n",
        "  ).transform(np.sum),\n",
        "    pct_monthly_prcp=lambda x: x.prcp.div(\n",
        "    x.total_prcp_in_month\n",
        "    )\n",
        ").nlargest(5, 'pct_monthly_prcp')"
      ],
      "metadata": {
        "id": "RcoCxdbcF3XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform() can be used on dataframes as well. We can use it to easily standardize the data:"
      ],
      "metadata": {
        "id": "_oGr90aZGFea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb[['open', 'high', 'low', 'close']].transform(\n",
        "  lambda x: (x - x.mean()).div(x.std())\n",
        ").head()"
      ],
      "metadata": {
        "id": "DBgyV0olGHUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pivot tables and crosstabs"
      ],
      "metadata": {
        "id": "QwbC3Q8rGHGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw pivots in before; however, we weren't able to provide any aggregations. With pivot_table() , we get the mean by default as the aggfunc . In its simplest form,\n",
        "we provide a column to place along the columns:"
      ],
      "metadata": {
        "id": "WReXcQ1NGL4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.pivot_table(columns='trading_volume')"
      ],
      "metadata": {
        "id": "pL4tFwCrGUW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By placing the trading volume in the index, we get the aggregation from the first example in the group by section above:"
      ],
      "metadata": {
        "id": "nsewOCWxGW3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.pivot_table(index='trading_volume')"
      ],
      "metadata": {
        "id": "_42KISOuGYZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With pivot() , we also weren't able to handle multi-level indices or indices with repeated values. For this reason we haven't been able to put the weather data in the wide\n",
        "format. The pivot_table() method solves this issue:"
      ],
      "metadata": {
        "id": "Ic5sEBFMII1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather.reset_index().pivot_table(\n",
        "  index=['date', 'station', 'station_name'],\n",
        "  columns='datatype',\n",
        "  values='value',\n",
        "  aggfunc='median'\n",
        ").reset_index().tail()"
      ],
      "metadata": {
        "id": "D76gigbOIKl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the pd.crosstab() function to create a frequency table. For example, if we want to see how many low-, medium-, and high-volume trading days Facebook\n",
        "stock had each month, we can use crosstab"
      ],
      "metadata": {
        "id": "51oiB23UIKM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(\n",
        "  index=fb.trading_volume,\n",
        "  columns=fb.index.month,\n",
        "  colnames=['month'] # name the columns index\n",
        ")"
      ],
      "metadata": {
        "id": "6yRNKikuIW_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can normalize with the row or column totals with the normalize parameter. This shows percentage of the total:"
      ],
      "metadata": {
        "id": "loEJIc-ILfNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(\n",
        "  index=fb.trading_volume,\n",
        "  columns=fb.index.month,\n",
        "  colnames=['month'],\n",
        "  normalize='columns'\n",
        ")"
      ],
      "metadata": {
        "id": "mbwcYIxoLfpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to perform a calculation other than counting the frequency, we can pass the column to run the calculation on to values and the function to use to aggfunc :"
      ],
      "metadata": {
        "id": "LzADvrh9N_j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(\n",
        "  index=fb.trading_volume,\n",
        "  columns=fb.index.month,\n",
        "  colnames=['month'],\n",
        "  values=fb.close,\n",
        "  aggfunc=np.mean\n",
        ")"
      ],
      "metadata": {
        "id": "ZNi1yAE5OABA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also get row and column subtotals with the margins parameter. Let's count the number of times each station recorded snow per month and include the subtotals"
      ],
      "metadata": {
        "id": "8pP7rieCOKTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snow_data = weather.query('datatype == \"SNOW\"')\n",
        "pd.crosstab(\n",
        "  index=snow_data.station_name,\n",
        "  columns=snow_data.index.month,\n",
        "  colnames=['month'],\n",
        "  values=snow_data.value,\n",
        "  aggfunc=lambda x: (x > 0).sum(),\n",
        "  margins=True, # show row and column subtotals\n",
        "  margins_name='total observations of snow' # name the subtotals\n",
        ")"
      ],
      "metadata": {
        "id": "pGIgb6xoOKpv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}