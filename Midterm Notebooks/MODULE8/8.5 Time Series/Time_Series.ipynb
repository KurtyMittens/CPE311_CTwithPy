{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series"
      ],
      "metadata": {
        "id": "tiHffb3yjmZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the Data"
      ],
      "metadata": {
        "id": "KlRBC_eLjpln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "n this notebook, we will be working with 5 data sets:\n",
        "- (CSV) Facebook's stock price daily throughout 2018 (obtained using the stock_analysis package).\n",
        "- (CSV) Facebook's OHLC stock data from May 20, 2019 - May 24, 2019 per minute from Nasdaq.com.\n",
        "- (CSV) melted stock data for Facebook from May 20, 2019 - May 24, 2019 per minute from Nasdaq.com.\n",
        "-(DB) stock opening prices by the minute for Apple from May 20, 2019 - May 24, 2019 altered to have seconds in the time from Nasdaq.com.\n",
        "- (DB) stock opening prices by the minute for Facebook from May 20, 2019 - May 24, 2019 from Nasdaq.com"
      ],
      "metadata": {
        "id": "h8zKp3JVjr4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "woYlMi_DkS1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y1504p1jJfA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "fb = pd.read_csv('/content/fb_2018.csv', index_col='date', parse_dates=True).assign(\n",
        "trading_volume=lambda x: pd.cut(x.volume, bins=3, labels=['low', 'med', 'high'])\n",
        ")\n",
        "fb.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time-based selection and filtering"
      ],
      "metadata": {
        "id": "cdQ30UbokhQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, when we have a DatetimeIndex , we can use datetime slicing. We can provide a range of dates. We only get three days back because the stock market is\n",
        "closed on the weekends:"
      ],
      "metadata": {
        "id": "OWBACugJkkcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb['2018-10-11':'2018-10-15']"
      ],
      "metadata": {
        "id": "NUPCGtbYkk55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fb['2018-q1'].equals(fb['2018-01':'2018-03'])"
      ],
      "metadata": {
        "id": "6I6AseOFkoRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first() method will give us a specified length of time from the beginning of the time series. Here, we ask for a week. January 1, 2018 was a holidayâ€”meaning the\n",
        "market was closed. It was also a Monday, so the week here is only four days:"
      ],
      "metadata": {
        "id": "0yySW97Ukt4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.first('1W')"
      ],
      "metadata": {
        "id": "dEHTZZqQkuNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last() method will take from the end"
      ],
      "metadata": {
        "id": "rQjCEy0ykymk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.last('1W')"
      ],
      "metadata": {
        "id": "rSpNaWABkzIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the next few examples, we need datetimes, so we will read in the stock data per minute file:"
      ],
      "metadata": {
        "id": "sKYW0r0ok5q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_per_minute = pd.read_csv(\n",
        "  '/content/fb_week_of_may_20_per_minute.csv', index_col='date', parse_dates=True,\n",
        "  date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d %H-%M')\n",
        ")\n",
        "stock_data_per_minute.head()"
      ],
      "metadata": {
        "id": "IkNa3eK0k58a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the Grouper to roll up our data to the daily level along with first and last"
      ],
      "metadata": {
        "id": "WFfzA2j7lIyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_per_minute.groupby(pd.Grouper(freq='1D')).agg({\n",
        "  'open': 'first',\n",
        "  'high': 'max',\n",
        "  'low': 'min',\n",
        "  'close': 'last',\n",
        "  'volume': 'sum'\n",
        "})"
      ],
      "metadata": {
        "id": "w3U9xZ0dlJQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The at_time() method allows us to pull out all datetimes that match a certain time. Here, we can grab all the rows from the time the stock market opens (9:30 AM):"
      ],
      "metadata": {
        "id": "CLj4RTV4lQYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_per_minute.at_time('9:30')"
      ],
      "metadata": {
        "id": "lSxyS9TclQzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use between_time() to grab data for the last two minutes of trading daily"
      ],
      "metadata": {
        "id": "SglQF6oBlXng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_per_minute.between_time('15:59', '16:00')"
      ],
      "metadata": {
        "id": "6fG8R2ltlX8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On average, are more shares traded within the first 30 minutes of trading or in the last 30 minutes? We can combine between_time() with Groupers and filter()\n",
        "from the aggregation.ipynb notebook to answer this question. For the week in question, more are traded on average around opening time than closing time:"
      ],
      "metadata": {
        "id": "kFqSiiOllbvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shares_traded_in_first_30_min = stock_data_per_minute\\\n",
        "  .between_time('9:30', '10:00')\\\n",
        "  .groupby(pd.Grouper(freq='1D'))\\\n",
        "  .filter(lambda x: (x.volume > 0).all())\\\n",
        "  .volume.mean()\n",
        "shares_traded_in_last_30_min = stock_data_per_minute\\\n",
        "  .between_time('15:30', '16:00')\\\n",
        "  .groupby(pd.Grouper(freq='1D'))\\\n",
        "  .filter(lambda x: (x.volume > 0).all())\\\n",
        "  .volume.mean()\n",
        "\n",
        "shares_traded_in_first_30_min - shares_traded_in_last_30_min"
      ],
      "metadata": {
        "id": "7hqR_WRflcJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cases where time doesn't matter, we can normalize the times to midnight:"
      ],
      "metadata": {
        "id": "nO-hHTHMljCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(\n",
        "  dict(before=stock_data_per_minute.index, after=stock_data_per_minute.index.normalize())\n",
        ").head()"
      ],
      "metadata": {
        "id": "KhtoTV3EljYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we can also use normalize() on a Series object after accessing the dt attribute:"
      ],
      "metadata": {
        "id": "jwzSAa8klnqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_per_minute.index.to_series().dt.normalize().head()"
      ],
      "metadata": {
        "id": "c98z9HRTln-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shifting for lagged data"
      ],
      "metadata": {
        "id": "PLQ0dlgDlq4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use shift() to create some lagged data. By default, the shift will be one period. For example, we can use shift() to create a new column that indicates the\n",
        "previous day's closing price. From this new column, we can calculate the price change due to after hours trading (after the close one day right up to the open the following\n",
        "day):"
      ],
      "metadata": {
        "id": "xOWU9sVbltjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.assign(\n",
        "  prior_close=lambda x: x.close.shift(),\n",
        "  after_hours_change_in_price=lambda x: x.open - x.prior_close,\n",
        "  abs_change=lambda x: x.after_hours_change_in_price.abs()\n",
        ").nlargest(5, 'abs_change')"
      ],
      "metadata": {
        "id": "XJX7rrjjlt92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tshift() method will shift the DatetimeIndex rather than the data. However, if the goal is to to add/subtract time we can use pd.Timedelta :"
      ],
      "metadata": {
        "id": "JSBvFe8BlzZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.date_range('2018-01-01', freq='D', periods=5) + pd.Timedelta('9 hours 30 minutes')"
      ],
      "metadata": {
        "id": "2tK7qZO6l3dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with stock data, we only have data for the dates the market was open. We can use first_valid_index() to give us the index of the first non-null entry in\n",
        "our data. For September 2018, this is September 4th:"
      ],
      "metadata": {
        "id": "hBJtGXQImDCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb['2018-09'].first_valid_index()"
      ],
      "metadata": {
        "id": "aioXPFxPmC6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversely, we can use last_valid_index() to get the last entry of non-null data. For September 2018, this is September 28th:"
      ],
      "metadata": {
        "id": "WV2mkssymH01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb['2018-09'].last_valid_index()"
      ],
      "metadata": {
        "id": "6lnu5iRKmKmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use asof() to find the last non-null data before the point we are looking for, if it isn't in the index. From the previous result, we know that the market was not open\n",
        "on September 30th. It also isn't in the index"
      ],
      "metadata": {
        "id": "ty0uiOOkmMB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.index.asof('2018-09-30')"
      ],
      "metadata": {
        "id": "IarbMxrtmOOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we ask for it, we will get the data from the index we got from fb['2018-09'].last_valid_index() , which was September 28th:"
      ],
      "metadata": {
        "id": "2sPIx_OGmWJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.asof('2018-09-30')"
      ],
      "metadata": {
        "id": "BH4UeoUvmhSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Differenced data"
      ],
      "metadata": {
        "id": "vp523vFTmrVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the diff() method is a quick way to calculate the difference between the data and a lagged version of it. By default, it will yield the result of data -\n",
        "data.shift()"
      ],
      "metadata": {
        "id": "BBY6HPgDmuWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "  fb.drop(columns='trading_volume')\n",
        "  - fb.drop(columns='trading_volume').shift()\n",
        ").equals(\n",
        "  fb.drop(columns='trading_volume').diff()\n",
        ")"
      ],
      "metadata": {
        "id": "lF4WtYXom0ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this to see how Facebook stock changed day-over-day"
      ],
      "metadata": {
        "id": "CBFMxk1Dmw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.drop(columns='trading_volume').diff().head()"
      ],
      "metadata": {
        "id": "ElW-jaYnm4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can specify the number of periods, can be any positive or negative integer"
      ],
      "metadata": {
        "id": "44B3L4U7m7n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.drop(columns='trading_volume').diff(-3).head()"
      ],
      "metadata": {
        "id": "e5BSGUSrm79o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resampling"
      ],
      "metadata": {
        "id": "9Relvgnzm_yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes the data is at a granularity that isn't conducive to our analysis. Consider the case where we have data per minute for the full year of 2018. Let's see what happens\n",
        "if we try to plot this.\n",
        "\n",
        "Plotting will be covered in the next module, so don't worry too much about the code.\n",
        "\n",
        "First, we import matplotlib for plotting"
      ],
      "metadata": {
        "id": "BOzUzE06nCRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nwbCtIUnm_l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will look at the plot at the minute level and at the daily aggregated level (summed):"
      ],
      "metadata": {
        "id": "m4IVdDfnnJ5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "index = pd.date_range('2018-01-01', freq='T', periods=365*24*60)\n",
        "raw = pd.DataFrame(\n",
        "  np.random.uniform(0, 10, size=index.shape[0]), index=index\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "raw.plot(legend=False, ax=axes[0], title='raw data')\n",
        "raw.resample('1D').sum().plot(legend=False, ax=axes[1], title='daily totals')\n",
        "for ax in axes:\n",
        "  ax.set_xlabel('date')\n",
        "  ax.set_ylabel('events')\n",
        "\n",
        "plt.suptitle('Raw versus Resampled Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BIV1ST3mnLRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot on the left has so much data we can't see anything. However, when we aggregate to the daily totals, we see the data. We can alter the granularity of the data we are\n",
        "working with using resampling. Recall our minute-by-minute stock data:"
      ],
      "metadata": {
        "id": "zWKODAffo4Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_per_minute.head()"
      ],
      "metadata": {
        "id": "zbF0bB2xo7zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can resample this to get to a daily frequency:"
      ],
      "metadata": {
        "id": "lSjzkCGtpALe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_per_minute.resample('1D').agg({\n",
        "  'open': 'first',\n",
        "  'high': 'max',\n",
        "  'low': 'min',\n",
        "  'close': 'last',\n",
        "  'volume': 'sum'\n",
        "})"
      ],
      "metadata": {
        "id": "vVyf6PxvpAnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can downsample to quarterly data:"
      ],
      "metadata": {
        "id": "S62gXe45pFdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.resample('Q').mean()"
      ],
      "metadata": {
        "id": "KzRe36JepE-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use apply() . Here, we show the quarterly change from start to end:"
      ],
      "metadata": {
        "id": "PzESLaeYpLlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.drop(columns='trading_volume').resample('Q').apply(\n",
        "  lambda x: x.last('1D').values - x.first('1D').values\n",
        ")"
      ],
      "metadata": {
        "id": "6rdD0NlnpMIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following melted stock data by the minute. We don't see the OHLC data directly"
      ],
      "metadata": {
        "id": "CXTa6EljpdJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "melted_stock_data = pd.read_csv('/content/melted_stock_data.csv', index_col='date', parse_dates=True)\n",
        "melted_stock_data.head()"
      ],
      "metadata": {
        "id": "6T2b-3T7pd21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the ohlc() method after resampling to recover the OHLC columns:"
      ],
      "metadata": {
        "id": "oCRRzT3Epr_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "melted_stock_data.resample('1D').ohlc()['price']"
      ],
      "metadata": {
        "id": "3vfARKEbpuYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can upsample to increase the granularity. Note this will introduce NaN values:"
      ],
      "metadata": {
        "id": "OhFo4rdtp01K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.resample('6H').asfreq().head()"
      ],
      "metadata": {
        "id": "pAWUxXzFp0d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many ways to handle these NaN values. We can forward-fill with pad()"
      ],
      "metadata": {
        "id": "9FCAenUOp8XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.resample('6H').pad().head()"
      ],
      "metadata": {
        "id": "PmHAbUIlp-la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can specify a specific value or a method with fillna() :"
      ],
      "metadata": {
        "id": "wAfT52f1qBDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.resample('6H').fillna('nearest').head()"
      ],
      "metadata": {
        "id": "LkdEDrMSqCS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use asfreq() and assign() to specify the action per column"
      ],
      "metadata": {
        "id": "XIu0gkSRqKBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb.resample('6H').asfreq().assign(\n",
        "  volume=lambda x: x.volume.fillna(0), # put 0 when market is closed\n",
        "  close=lambda x: x.close.fillna(method='ffill'), # carry forward\n",
        "  # take the closing price if these aren't available\n",
        "  open=lambda x: np.where(x.open.isnull(), x.close, x.open),\n",
        "  high=lambda x: np.where(x.high.isnull(), x.close, x.high),\n",
        "  low=lambda x: np.where(x.low.isnull(), x.close, x.low)\n",
        ").head()"
      ],
      "metadata": {
        "id": "excC127ZqKVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging"
      ],
      "metadata": {
        "id": "6cwa7BnXqPDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw merging examples the querying_and_merging notebook. However, they all matched based on keys. With time series, it is possible that they are so granular that we\n",
        "never have the same time for multiple entries. Let's work with some stock data at different granularities:"
      ],
      "metadata": {
        "id": "CAwXst_OqQF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "with sqlite3.connect('/content/stocks.db') as connection:\n",
        "  fb_prices = pd.read_sql(\n",
        "    'SELECT * FROM fb_prices', connection,\n",
        "    index_col='date', parse_dates=['date']\n",
        "  )\n",
        "  aapl_prices = pd.read_sql(\n",
        "    'SELECT * FROM aapl_prices', connection,\n",
        "    index_col='date', parse_dates=['date']\n",
        "  )"
      ],
      "metadata": {
        "id": "ZYqAvtwsqTZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Facebook prices are at the minute granularity"
      ],
      "metadata": {
        "id": "BB4Pi5J8qkGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fb_prices.index.second.unique()"
      ],
      "metadata": {
        "id": "ktBtoOBmql1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the Apple prices have information for the second"
      ],
      "metadata": {
        "id": "8boYdbyQqnEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aapl_prices.index.second.unique()"
      ],
      "metadata": {
        "id": "WFAmV9yaqoQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform an asof merge to try to line these up the best we can. We specify how to handle the mismatch with the direction and tolerance parameters. We will\n",
        "fill in with the direction of nearest and a tolerance of 30 seconds. This will place the Apple data with the minute that it is closest to, so 93152 will go with 932\n",
        "and 93707 will go with 937. Since the times are on the index, we pass left_index and right_index , as we did with merges earlier this chapter"
      ],
      "metadata": {
        "id": "kTntWNciqq2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.merge_asof(\n",
        "  fb_prices, aapl_prices,\n",
        "  left_index=True, right_index=True, # datetimes are in the index\n",
        "  # merge with nearest minute\n",
        "  direction='nearest', tolerance=pd.Timedelta(30, unit='s')\n",
        ").head()"
      ],
      "metadata": {
        "id": "fSm7IMl3qtJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don't want to lose the seconds information with the Apple data, we can use pd.merge_ordered() instead, which will interleave the two. Note this is an outer join by\n",
        "default ( how parameter). The only catch here is that we need to reset the index in order to join on it:"
      ],
      "metadata": {
        "id": "xt9ClLvNqxOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.merge_ordered(\n",
        "  fb_prices.reset_index(), aapl_prices.reset_index()\n",
        ").set_index('date').head()"
      ],
      "metadata": {
        "id": "CQZUd-0Nqx-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can pass a fill_method to handle NaN values"
      ],
      "metadata": {
        "id": "8Tsv_Gnlq2DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.merge_ordered(\n",
        "  fb_prices.reset_index(), aapl_prices.reset_index(),\n",
        "  fill_method='ffill'\n",
        ").set_index('date').head()"
      ],
      "metadata": {
        "id": "_3bczeutq2ad"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}